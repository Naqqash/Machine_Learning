{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making sure that the features have proper data format\n",
    "dtype_dict = {'bathrooms':float, 'waterfront':int, 'sqft_above':int, 'sqft_living15':float, 'grade':int, 'yr_renovated':int, 'price':float, 'bedrooms':float, 'zipcode':str, 'long':float, 'sqft_lot15':float, 'sqft_living':float, 'floors':str, 'condition':int, 'lat':float, 'date':str, 'sqft_basement':int, 'yr_built':int, 'id':str, 'sqft_lot':int, 'view':int}\n",
    "\n",
    "# Reading the csv files using panda\n",
    "data_csv = pd.read_csv('kc_house_data.csv',dtype=dtype_dict)\n",
    "train_csv = pd.read_csv('kc_house_train_data.csv',dtype=dtype_dict)\n",
    "test_csv = pd.read_csv('kc_house_test_data.csv',dtype=dtype_dict)\n",
    "\n",
    "# Conversting the csv data to the dataframe and deleting the extra columns which are not relevant\n",
    "\n",
    "house_train = pd.DataFrame(train_csv)\n",
    "train_data = house_train.drop(['price','id','date'],axis=1)\n",
    "house_test = pd.DataFrame(test_csv)\n",
    "test_data  = house_test.drop(['price','id','date'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(data_sframe, features, output):\n",
    "    data_sframe['constant'] = 1 # this is how you add a constant column to an SFrame\n",
    "    # add the column 'constant' to the front of the features list so that we can extract it along with the others:\n",
    "    features = ['constant'] + features # this is how you combine two lists\n",
    "    # select the columns of data_SFrame given by the features list into the SFrame features_sframe (now including constant):\n",
    "    features_sframe = data_sframe[features]\n",
    "    # the following line will convert the features_SFrame into a numpy matrix:\n",
    "    feature_matrix = features_sframe.as_matrix()\n",
    "    # assign the column of data_sframe associated with the output to the SArray output_sarray\n",
    "    #output_sarray = pd.DataFrame(output)\n",
    "    # the following will convert the SArray into a numpy array by first converting it to a list\n",
    "    output_array = output.as_matrix()\n",
    "    return(feature_matrix, output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e+00   1.18000000e+03]\n",
      "221900.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(example_features,example_output) = get_numpy_data(train_data, ['sqft_living'], train_csv.price) # the [] around 'sqft_living' makes it a list\n",
    "print example_features[0,:] # this accesses the first row of the data the ':' indicates 'all columns'\n",
    "print example_output[0] # and the corresponding output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to predict the valus from the features and weights\n",
    "def predict_output(feature_matrix, weights):\n",
    "    # assume feature_matrix is a numpy matrix containing the features as columns and weights is a corresponding numpy array\n",
    "    # create the predictions vector by using np.dot()\n",
    "    predictions = np.dot(feature_matrix,weights)\n",
    "\n",
    "    return(predictions)  \n",
    "#funtion for the derivative for gradient descent\n",
    "def feature_derivative(errors, feature):\n",
    "    # Assume that errors and feature are both numpy arrays of the same length (number of data points)\n",
    "    # compute twice the dot product of these vectors as 'derivative' and return the value\n",
    "    derivative =  2*np.dot(feature,errors)\n",
    "    return(derivative)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[-221900. -538000. -180000. ..., -360000. -400000. -325000.]\n",
      "[ 1.  1.  1. ...,  1.  1.  1.]\n",
      "-18752698920.0\n",
      "-18752698920.0\n"
     ]
    }
   ],
   "source": [
    "(example_features, example_output) = get_numpy_data(train_data, ['sqft_living'], train_csv.price) \n",
    "my_weights = np.array([0., 0.]) # this makes all the predictions 0\n",
    "print my_weights\n",
    "\n",
    "test_predictions = predict_output(example_features, my_weights) \n",
    "\n",
    "print test_predictions\n",
    "# just like SFrames 2 numpy arrays can be elementwise subtracted with '-': \n",
    "errors = test_predictions - example_output # prediction errors in this case is just the -example_output\n",
    "print errors\n",
    "feature = example_features[:,0] # let's compute the derivative with respect to 'constant', the \":\" indicates \"all rows\"\n",
    "print feature\n",
    "derivative = feature_derivative(errors, feature)\n",
    "print derivative\n",
    "print -np.sum(example_output)*2 # should be the same as derivative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance):\n",
    "    converged = False \n",
    "    weights = np.array(initial_weights) # make sure it's a numpy array\n",
    "    while not converged:\n",
    "        # compute the predictions based on feature_matrix and weights using your predict_output() function\n",
    "        predictions = predict_output(feature_matrix,weights)\n",
    "        \n",
    "        # compute the errors as predictions - output\n",
    "        errors = predictions - output\n",
    "\n",
    "        gradient_sum_squares = 0 # initialize the gradient sum of squares\n",
    "        # while we haven't reached the tolerance yet, update each feature's weight\n",
    "        for i in range(len(weights)): # loop over each weight\n",
    "            # Recall that feature_matrix[:, i] is the feature column associated with weights[i]\n",
    "            # compute the derivative for weight[i]:\n",
    "            \n",
    "            derivative = feature_derivative(feature_matrix[:,i],errors)\n",
    "            \n",
    "\n",
    "            # add the squared value of the derivative to the gradient sum of squares (for assessing convergence)\n",
    "            gradient_sum_squares = gradient_sum_squares + derivative*derivative\n",
    "            # subtract the step size times the derivative from the current weight\n",
    "            weights[i] = weights[i] - step_size*derivative\n",
    "            \n",
    "        # compute the square-root of the gradient sum of squares to get the gradient matnigude:\n",
    "        gradient_magnitude = sqrt(gradient_sum_squares)\n",
    "        if gradient_magnitude < tolerance:\n",
    "            converged = True\n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running the Gradient Descent as Simple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The learned weights for the single feature 'sqft_living' through GB are: [-46999.88716555    281.91211918]\n",
      "The predicted price for the first element is: 356134.443255\n",
      "RSS of the simple Regression using GB is: 2.75400044902e+14\n"
     ]
    }
   ],
   "source": [
    "simple_features = ['sqft_living']\n",
    "(simple_feature_matrix, output) = get_numpy_data(train_data, simple_features, train_csv.price)\n",
    "initial_weights = np.array([-47000., 1.])\n",
    "step_size = 7e-12\n",
    "tolerance = 2.5e7\n",
    "## Calling the regression funtion on the values\n",
    "\n",
    "simple_reg_weights = regression_gradient_descent(simple_feature_matrix,output,initial_weights,step_size,tolerance)\n",
    "\n",
    "# By using the learned weights predicting value on the test data\n",
    "(test_simple_feature_matrix, test_output) = get_numpy_data(test_data, simple_features, test_csv.price)\n",
    "\n",
    "model1_predicted_output = predict_output(test_simple_feature_matrix,simple_reg_weights)\n",
    "\n",
    "print \"The learned weights for the single feature 'sqft_living' through GB are: \" + str(simple_reg_weights)\n",
    "print \"The predicted price for the first element is: \" + str(model1_predicted_output[0])\n",
    "\n",
    "# Calculating RSS of the model\n",
    "\n",
    "error_model1 = model1_predicted_output - test_output\n",
    "RSS = sum(error_model1*error_model1)\n",
    "print \"RSS of the simple Regression using GB is: \" + str(RSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Multiple Regression using Gradient Desent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_features = ['sqft_living', 'sqft_living15'] # sqft_living15 is the average squarefeet for the nearest 15 neighbors. \n",
    "my_output = 'price'\n",
    "(feature_matrix, output) = get_numpy_data(train_data, model_features, train_csv.price)\n",
    "initial_weights = np.array([-100000., 1., 1.])\n",
    "step_size = 4e-12\n",
    "tolerance = 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multi_reg_weights = regression_gradient_descent(feature_matrix,output,initial_weights,step_size,tolerance)\n",
    "(test_multi_feature_matrix, test_output) = get_numpy_data(test_data,model_features,test_csv.price)\n",
    "model2_predicted_output = predict_output(test_multi_feature_matrix,multi_reg_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The learned weights for the features 'sqft_living and sqft_living15' through GB are: [ -9.99999688e+04   2.45072603e+02   6.52795267e+01]\n",
      "The predicted price for the first element is: 366651.411629\n",
      "RSS of the simple Regression using GB is: 2.7026344363e+14\n"
     ]
    }
   ],
   "source": [
    "print \"The learned weights for the features 'sqft_living and sqft_living15' through GB are: \" + str(multi_reg_weights)\n",
    "\n",
    "print \"The predicted price for the first element is: \" + str(model2_predicted_output[0])\n",
    "\n",
    "# Calculating RSS of the model\n",
    "\n",
    "error_model2 = model2_predicted_output - test_output\n",
    "RSS = sum(error_model2*error_model2)\n",
    "print \"RSS of the multiple Regression using GB is: \" + str(RSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
