{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "## For plotting\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '/Users/naqqashabbassi/Documents/Machine_Learning_Datasets/Udacity/Deep_Learning/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization for a normal logistic regression implementation from Assignment 2 with SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 18.770330\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 11.8%\n",
      "Minibatch loss at step 500: 2.473430\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 1000: 1.815637\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 1500: 0.981581\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 2000: 0.859596\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 2500: 0.806492\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 3000: 0.774936\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 88.9%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  beta_regularlization = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))+ beta_regularlization * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction =  tf.nn.softmax(tf.matmul(tf_test_dataset, weights)  + biases)\n",
    "    \n",
    "    \n",
    "    \n",
    "#############################################################################################################\n",
    "\n",
    "num_steps = 3001\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,beta_regularlization : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with beta value of  0\n",
      "Initialized with beta value of  0.001\n",
      "Initialized with beta value of  0.002\n",
      "Initialized with beta value of  0.003\n",
      "Initialized with beta value of  0.004\n",
      "Initialized with beta value of  0.005\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [0,0.001,0.002,0.003,0.004,0.005]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.initialize_all_variables().run()\n",
    "      print(\"Initialized with beta value of \",regul)\n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,beta_regularlization : regul}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmMAAAFCCAYAAAC90NpzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xu8ZXP9+PHXm3GbyJAkSuebS5QyhKhvNaJy+Ub1ra/8\npIYvXeiiFE3KjEu5hCJKklBuKZVvKZcykmKSGYRxiWMwSMa1BmPm/ftjrcOa7dxmZu+zzjrn9Xw8\n9uPstfe6vNd+7zPnPZ/Pe68dmYkkSZLqsVTdAUiSJI1mFmOSJEk1shiTJEmqkcWYJElSjSzGJEmS\namQxJkmSVCOLMUltFRELIuLVdcexqJYk7ohYOyKeiIhoc0xvjYiZ7dxny/6vioiNy/tTIuJHbdjn\nYsccERdFxO6DWO+aiHjt4hxDGo4sxjQqRcST5R/PJ8o/wv+uLO+6GPubGhH/24lYNfxl5qzMXCmX\n8MKNrQVhZl6ZmRsseYS9Hus9wGOZeX3P4dqx38HG3Fvxl5k7ZOZgCsJjgEMXN0ZpuLEY06iUmSuW\nfzxXAu4G/qtnOTPPWZxdtjnEtoqIMXXHsKQiYum6Y+hNB17bto6u9eMTQLXwGarjtsP/AVtHxMvq\nDkRqB4sxqSIiloqIL0XEHRHxz4g4LyJWKZ9bPiJ+XD7+SERMi4jVI+JrwFuBE8uRtRP62Pf5EXF/\nRDwaEVdUp1kiYoWIODYiusvnr4yI5cvn/jMi/lQec1ZEfKR8fKHRuIiYGBFXVpYXRMQ+EXE7cGv5\n2PHlPh6LiGsj4j9bzv3L5bk/Xj7/iog4KSKOaTmXCyNiv35eyh0j4u8R8VBEHB2FZSNiTkRsVNnP\n6hHxr4h4SS+v18RyGu24iPgnMLncxzERcXdEPBAR3+15ncptDoiI2RFxb0TsVR1pGuj1ajn2jhEx\nvXydZkXE5MpzXeV+94yIu4HLIuJV5WNLRcRWlVHWJyLiqYi4q9x2i4j4c5nL2RHx7YhYpnzuD+Uh\nri+3+2BETIiIeyrH3rA8j0ci4m9RjG71PHd6matflfm7OvqYdo2IZYGtgSv6SmBE7BQRN5XHujwi\nNqg8t2n5+jweET+J4vfksPK51pgPLPPxeETMjIh3RMR2wCRgl/Jcp/eRo70j4uZy25siYhOAzHwK\n+Cvw7r7il5rEYkxa2KeBnYC3AS8HHgFOKp/7KPBi4BXAqsDHgbmZeRBwJbBvObL2mT72/WtgXeCl\nwHXAWZXnjgE2AbYq9/1FYEFEvAq4CDgeWA0YD1SnlQYakdsZ2BzoKfymARsDqwBnA+eXf5gB9gc+\nBGyfmS8G9gD+DZwO7BpR9ENFxGrANi3xt3ov8EZg0zKGPTPzGeAc4MOV9XYFLsvMh/vYzxbA34HV\nga8DR1G8hhuXP9cCDi7j2g74XBnbesCEln0N5vXq8STw4cxcGdgR+GRE7NyyztuADSgKgudGlTLz\nz5VR11WAqylea4Bngc8CL6HI9TbAPuV2byvXeUO5/fnVg5VF2/8Bv6V4D30aOCsi1q+stgswpTzu\nHcDX+ji/9YAFmTm7tyfLfZ4NfIbifXcR8H8RMaZ8v/wcOK08zjkU+X7BaxsRrwH2BTYr31PvAroz\n87cU+Ty3PNdNel6+nv1ExAeBycDu5bbvAarvk1so3gdS41mMSQv7OPCVzJydmfOAQ4APRDFF9gzF\nH9H1sjA9M5+obNvvNE9mnp6Z/6rsd+OIWCkilqIofD6bmfdn5oLMvLosXv4fcGlmnpeZ8zNzTqXH\nZzCOyMxHM/PpMoazMvOR8hjHAcsBrynX3Qs4KDNvL9e9sTzeX4DHKAoHKAq2yzPzoX6Oe1R53HuA\nb1EUXQBnVu4D7M7CU2WtZmfmSZm5AHga2Bv4fLnvJ4EjyngA/gc4LTNvycy5FH/IF0tmXpGZN5X3\nbwTOBd7estqUzJzb89r24dvA42XBTmZel5nTytf/buCUXvbbly2BF2XmkZn5bGZeDvyKhV/PCzLz\n2sycT1Esj+9jX+OAJ/p4Doqi7leZ+btyX8cAKwBvKeNYOjO/Xb4nf05R5PdmPsV77HURsUzZW3dn\n+VzQ/+/MXhTvo78CZOadmTmr8vwT5XlIjWcxJi2sC/h5OTXzCHAzxWjG6hRFw8XAuRFxX0QcFQv3\nC/U56lJOXx0ZxRTgY8Bd5VOrlbflKUaAWr0CuLOXxwfrnupCRHyhnPZ5tDy/lcvj9xyrtxigKKJ6\nRrQ+TP8FVOtxZwFrAmTmNcDcciprA2Ad4MJB7uelwFjgr5X8/KYS/8tb1r93gBj7FBFvKqfm/hER\nj1IU6a1Tqff0sml1Hx+nGD37f5XH1i+nEe8v3wdf62W/fVmzl2PeXT4Oxfvvwcpzc4EV+9jXI8BK\nAxzrucKn/GDCPRQjkS8H7mtZv9fXIjPvAPajGK17MCLOiYiX93Pcqv7ej1CMUj8yyH1Jw5rFmLSw\nWcB2mblK5Ta2HLF6NjMPzczXAW8G/gv4SLndQNNfu1FMf25TTn39R/l4AP8EnqKYdmt1D0XB0pt/\nAS+qLK/RyzrPxRURb6WY/vxgZo7LzFUoRrx6Rifu6SMGgB8DO0dxGYQNgF/0sV6PtVvuV/94n0FR\n0O0OnF+OAPal+rr+k6LAeG0lN+PKKSyA+4FXVtav3ofBvV49zqY4x1dk5jjgZF7472V/xfdbKT7t\nt3M5gtfjuxQF/rrl++CgXvbbl9nAK3umi0uv4oWF0WDcUYTZZ2F0X7lvKFekeD3vpXid12pZf236\nkJnnZOZby/0lxVQzDPw709/7EWBDnp+ylxrNYkxa2MnA1yNibYCIeGlE7FTenxARry+nLJ8A5lFM\nw0AxItFX0QTFCMXTwJyIeBFFvwwA5RTcacBxEfHyiFi6bAJflmKqaduymXtMRLykLIgAZgDvj6L5\nf11goEtrrEQxyvfPKBrhD6YYXehxKnBYRKwbhTdExKpljPcC11KMkP10gKk5gC9ExLiIeCVF39F5\nled+DLyfokA9c4D9PKd8nb4PfCsiXgoQEWtFxLvKVX4C7BERG0TEWOCrLbtYlNdrReCRzHwmIrag\nGN0aVL9Zec4/oeh1uqOX/T4B/LscGfxky/P9vY+uoejhOyAilomICRT/ITi359CDiQ+gLIAv44V9\ndT3Op/gQxjvKXrX9Kf7D8CeKHrj5EfGp8j3Z05f4AuVI4DsiYjmK9/9TPP878wDQ1VJcVp1K8T7a\ntHw/rlv5vVyeoh/x0sGeszScWYxJCzueYtrskoh4HPgzRRM5FCMp51OMJt0MTOX56brjKXrL5kTE\nt3rZ75kUU0r3AX8r91v94/4F4EbgLxRNykcAS5U9VztQ/DF8GJgOvKHc5psUfWwPAj+kKHKq+2wt\nHn5b3m4DuilGmao9OMdRFBGXlOf4fYrp0x5nAK9n4ClKgF9SfNptOkVf02nPBVWc03UUDeR/7Gcf\nvTXcH0gxqnN1Oc13KbB+ud/fAicAl5fn+Odym57CcVFer32AQ8v3wFdZuJhsXbf1sW0oprV/Fs9/\novLG8rkvUBR2j1P0i53bsq8pwBnlNOwHqq9BWUC9B9geeAg4kaLgu61y/Na4+isgv0cxOlldt+dY\nt1KMXn67PNaOwHvK0eFnKIrp/6WYJtyNIsfPtOwLin6xI8p93E8xpTypfK7nAwoPR8S1rcFl5k8p\npnHPpni9LqD4wADl63B5Zj7Qz/lJjRE5wDUKI2ISxS/lAoo/FntQTFOcTDHk3w3s1tLI3LNtN8Uv\n0XxgXmZu0bqOpGYop95+nJmvGnDlgff1A+C+zDx4ySPr8xgbUvybtWw5qqYWEfFHik8BL9F0X0Rc\nA3wnM89oT2QDHu9qik/o3jwUx5M6rd9iLCK6gN8DG2bm0xFxHsVHnPcF9s/MKyNiD+A/evtHNYpr\n67wxM+d0InhJQ6OcqjoXmJ6Zhy/hvrooRszGl58obJuIeB/Fv1FjKUbyns3M97fzGIKIeBvF6OM/\nKUbGvgO8OjMf7HdDSb0aaJrycYq+mLHlp8bGUjSRrp+ZPRdLvAz473720aSrOktqUY4wPQK8jOIy\nFUuyr8MoRquObnchVvoYxTTkHRT/drX2ZKk9XkPRg/cIxbXdPmAhJi2+wUxTfgw4lqK/5OLM3D0i\nrqL4x/SXEfF5iuvtvLiXbe+k6D2ZD3wvM7/f9jOQJElqsH5HxiJiHYprxHRRXHdmxYjYDdgT2Kds\nulyRhRs3q96SxZWVtwf2LXtOJEmSVBroC243A/6U5VeVRMQFwJsz8yzK7wSL4mszduxt48y8v/z5\nUET8nOJTaQt9F1xEDOsvWJYkSarKzLa2YA3UMzYT2LK8Lk8A2wI3V67xsxTwFYoLGS4kIsZGxErl\n/RdRfCfZja3rAWSmt4beJk+eXHsM3szdaLyZv2bfzF9zb53QbzGWxcedz6S42OMN5cPfB/5fRNxK\n8UWt92bm6QARsWZE/Lpcbw3gyoiYQXGxwl9l5iXtPwXVqbu7u+4QtJjMXbOZv2Yzf6oaaJqSzDwa\nOLrl4ePLW+u6symnLLP4Mti+vqRWkiRJeAV+LaGJEyfWHYIWk7lrNvPXbOZPVQNe2qLjAURk3TFI\nkiQNRkSQQ9zAL/Vr6tSpdYegxWTums38NZv5U5XFmCRJUo2cppQkSRokpyklSZJGGIsxLRH7HprL\n3DWb+Ws286cqizFJkqQa2TMmSZI0SPaMSZIkjTAWY1oi9j00l7lrNvPXbOZPVRZjkiRJNbJnTJIk\naZDsGZMkSRphLMa0ROx7aC5z12zmr9nMn6osxiRJkmpkz5gkSdIg2TMmSZI0wliMaYnY99Bc5q7Z\nzF+zmT9VWYxJkiTVyJ4xSZKkQbJnTJIkaYSxGNMSse+hucxds5m/ZjN/qrIYkyRJqpE9Y5IkSYPU\niZ6xMe3cmUaXv/wFpk+HZZeFZZZ5/mf1/qL8XHrpus9IkqSh58iYFssTT8C668Kmm07lZS+bwDPP\nwLx5LNFPWLwibjj8jLb+H2loTJ06lQkTJtQdhhaT+Ws289dcjoxp2DjxRNhmG/jYx6Bd/57Mn79k\nxdxAP5966oWPt2Pf8+bBmDG9F2nDoVDs6+f8+e3JmyRpyTgypkX2+OPFqNgf/gAbbFB3NPXLhGef\n7WwhuajF4WDWfeaZojBbaaXituKKz9/v67GB1hnjf+8kjXCOjGlYOOEEePe7LcR6RDw/CtYkmTB3\nbjHl3HN78smFl3see/hh6O4eeL2e4m4whZ3FnSQVHBnTInnssWJU7KqrYP317XtosnbnblGKu8E+\nZnHXN3/3ms38NVctI2MRMQn4MLAAuBHYA9gAOBl4EdAN7JaZT/Sy7XbAt4ClgVMz86i2Ra5afOtb\nsMMORSEmVUXA2LHF7WUvW/L9tRZ3/RVx1ZG7/tZbdtn2FXYjrbiTVJ9+R8Yiogv4PbBhZj4dEecB\nFwH7Avtn5pURsQfwH5l5cMu2SwO3AtsC9wF/AXbNzFta1nNkrCEefbQYFbv66uKn1CSLUtwN9jGL\nO2n0qWNk7HFgHjA2IuYDY4HZwPqZeWW5zmXAb4GDW7bdArgjM7sBIuJcYGfgFtRI3/wm7LSThZia\naTiM3A1U7C2zTPsKO4s7qTn6/VXNzDkRcSwwC5gLXJyZl0bETRGxc2b+Evgg8MpeNl8LuKeyfC/w\npjbFrSE2Zw6cdBJMm7bw4/Y9NJe5WzKdLO4GKtrmzIE//WkqK688oS3FnSN3Q8/fP1X1+6sVEesA\n+wFdwGPA+RGxG7AncEJEfBW4EHiml82dexxBjjsO3vc+ePWr645EGpkWtbibOrX/a/wtSnE3mJG7\nJ58sijunZaX2G+hXYTPgT5n5MEBEXAC8OTPPAt5dPrY+sGMv297HwiNmr6QYHXuBiRMn0tXVBcC4\nceMYP378c/9j6Plme5frW37sMfjudyfw17++8PmedYZTvC4PbnnChAnDKh6X25u/CJg27YXPjx0L\nO+yw6MfLhIsvnsrcufCGNxQjcn/4w1T+/W949auL5enTpzJ7Nqy22gTuvhtuv71Yf7nliucffLBY\nf968CTz5JCy11FRWWAFe8pIJrLQSLFhQLL/qVcXyY48V8W60UbF8zz3F8lZbFcu33FKs/653Fct/\n/OPwyc9Ay/7+NWe55353dzedMlAD/8bAWcDmwFPA6cA04CeZ+VBELFU+9vvMPL1l2zEUDfzbUPSZ\nTcMG/kaaNAkeeQROPrnuSCSNFD0jd+34IEV15M5LoajTOtHAP+B1xiLiAOCjFJe2uA7YG/gksE+5\nys8y88vlumsC38/MHcvl7Xn+0hY/yMwjetm/xdgw9tBDxcVdp0+Htdd+4fNTK6NiahZz12zmb2GZ\nxVeetaOwW9zibqB1q8Wd+WuuWq4zlplHA0e3PHx8eWtddzaVKcvM/A3wmyWMUTX6xjfgQx/qvRCT\npOEiAlZYobitvvqS729Rirs5c+DuuxetuFuwAFZdFZZbDpZfvvjZc6suD+b+oqy31FJL/tqo/bwC\nv/r04IPw2tfC9dfDK15RdzSS1Fytxd1TT8HTTxe36v3W5UW9P9BzY8Z0vugb7DZNLQxrmabsNIux\n4Wv//Ysvk/72t+uORJK0pDJh3rz2FHZLWiT2VxgOVaG4uIWhXxSuIfPAA/DDH8Lf/tb/evY9NJe5\nazbz12x15C+i+NaIZZctpkvrtKiFYX+F3ZNPLvno4Zgxgy/gOsFiTL066ij4yEdgzTXrjkSSNNI0\nuTC88ML2x+A0pV5g9mzYaCO4+WZYY426o5EkafiwZ0xD4jOfKT71c+yxdUciSdLw0olirKGfZVCn\n3HsvnHUWHHDA4NavXqFYzWLums38NZv5U5XFmBZyxBHwv//bni8+liRJA3OaUs+ZNQs22QRmzoSX\nvrTuaCRJGn6cplRHff3rsPfeFmKSJA0lizEBxVd5nH8+fOELi7adfQ/NZe6azfw1m/lTlcWYAPja\n1+ATn4DVVqs7EkmSRhd7xsRdd8Hmm8NttxVfXCtJknpnz5g64vDDYZ99LMQkSaqDxdgod8cd8Mtf\nwuc+t3jb2/fQXOau2cxfs5k/VVmMjXKHHw6f/jSsskrdkUiSNDrZMzaK3X47vPnNxejYyivXHY0k\nScOfPWNqq0MPhc9+1kJMkqQ6WYyNUjNnwsUXF18KviTse2guc9ds5q/ZzJ+qLMZGqUMPhf32gxe/\nuO5IJEka3ewZG4Vuvhm23rroFVtppbqjkSSpOewZU1sceih8/vMWYpIkDQcWY6PM3/4Gl18O++7b\nnv3Z99Bc5q7ZzF+zmT9VWYyNMoccAl/8Iqy4Yt2RSJIksGdsVLnhBnj3u4tesRe9qO5oJElqHnvG\ntESmTIEDDrAQkyRpOLEYGyWmT4err4ZPfKK9+7XvobnMXbOZv2Yzf6qyGBslpkyBAw+EFVaoOxJJ\nklRlz9go8Ne/ws47F99FaTEmSdLis2dMi2XKFPjSlyzEJEkajizGRrhp02DGDNhrr87s376H5jJ3\nzWb+ms38qWrAYiwiJkXETRFxY0ScHRHLRcQWETEtIqZHxF8iYvM+tu2OiBvK9aa1P3wNZMoU+PKX\nYfnl645EkiT1pt+esYjoAn4PbJiZT0fEecBFwETgyMy8OCK2Bw7IzK172f4u4I2ZOaefY9gz1iF/\n/jN86ENw222w3HJ1RyNJUvN1omdszADPPw7MA8ZGxHxgLDAbeABYuVxnHHBfP/toa8AavClT4KCD\nLMQkSRrO+p2mLEe0jgVmURRhj2bmpcCXgGMjYhbwDWBSX7sALouIayNi7/aFrYFcdVUxIjZxYmeP\nY99Dc5m7ZjN/zWb+VNVvMRYR6wD7AV3AmsCKEbEb8APgM5m5NvA54LQ+dvGWzNwE2B7YNyLe2q7A\n1b/Jk+ErX4Fll607EkmS1J+BesZ2Ad6ZmXuVy7sDWwEfzswXl48FxYjZyn3uqFhvMvBkZh7b8nh+\n9KMfpaurC4Bx48Yxfvx4JkyYADz/vweXB798/fVwwgkTmDkTrrqq/nhcdtlll112uanLPfe7u7sB\nOOOMM9reMzZQMbYxcBawOfAU8EPgWmAP4POZeUVEbEPRzL95y7ZjgaUz84mIeBFwCXBIZl7Ssp4N\n/G229dbwkY/AHnvUHYkkSSPLkF/0NTOvB86kKMBuoGjG/x7wceDoiJgBHA58rAxwzYj4dbn5GsCV\n5TrXAL9qLcTUflOnwj33wO67D9Xxpg7NgdR25q7ZzF+zmT9VDfRpSjLzaODoloevBd7Uy7qzgR3L\n+3cC49sQowYps+gVO/hgGDNgZiVJ0nDgd1OOIL/7HXzyk3DzzRZjkiR1gt9NqT71jIpNnmwhJklS\nk1iMjRCXXQYPP1xccX8o2ffQXOau2cxfs5k/VVmMjQCZRZ/Y5Mmw9NJ1RyNJkhaFPWMjwG9/C/vv\nDzfcYDEmSVIn2TOmF3BUTJKkZrMYa7iLLoK5c+EDH6jn+PY9NJe5azbz12zmT1UWYw3W8wnKKVNg\nKTMpSVIj2TPWYBdeWExRXnedxZgkSUPBnjE9x1ExSZJGBv+MN9QvfgERsPPO9cZh30NzmbtmM3/N\nZv5U5bXaG2jBgmJE7PDDi4JMkiQ1lz1jDfSzn8GRR8K0aRZjkiQNpU70jDky1jA9o2JHHmkhJknS\nSGDPWMP89KcwdizssEPdkRTse2guc9ds5q/ZzJ+qHBlrkPnzi1Gx445zVEySpJHCnrEGOecc+Pa3\n4aqrLMYkSapDJ3rGLMYaYv58eN3rimLsne+sOxpJkkYnL/o6ip1zDqy2Gmy7bd2RLMy+h+Yyd81m\n/prN/KnKnrEGePZZOPRQOPlkpyclSRppnKZsgDPPhB/8AKZOtRiTJKlO9oyNQs8+CxtsUBRjb397\n3dFIkjS62TM2Cv3oR7D22sO3ELPvobnMXbOZv2Yzf6qyZ2wYmzcPDjsMzjij7kgkSVKnOE05jJ16\nKpx3Hlx6ad2RSJIksGdsVHnmGVh/fTj7bHjzm+uORpIkgT1jo8oPf1g07g/3Qsy+h+Yyd81m/prN\n/KnKnrFh6Omn4Wtfg/PPrzsSSZLUaU5TDkPf+Q786ldw0UV1RyJJkqrsGRsFnnoK1lsPLrgANt+8\n7mgkSVKVPWOjwKmnwvjxzSnE7HtoLnPXbOav2cyfqgYsxiJiUkTcFBE3RsTZEbFcRGwREdMiYnpE\n/CUiei0dImK7iJgZEbdHxIHtD39kmTsXjjgCpkypOxJJkjRU+p2mjIgu4PfAhpn5dEScB1wETASO\nzMyLI2J74IDM3Lpl26WBW4FtgfuAvwC7ZuYtLes5TVk6/ni4/HL4xS/qjkSSJPWmE9OUA32a8nFg\nHjA2IuYDY4HZwAPAyuU64yiKrVZbAHdkZjdARJwL7Azc0su6o96//w1HHgm/+U3dkUiSpKHU7zRl\nZs4BjgVmURRhj2bmpcCXgGMjYhbwDWBSL5uvBdxTWb63fEy9OPnk4ppi48fXHcmise+hucxds5m/\nZjN/qup3ZCwi1gH2A7qAx4DzI2I3YA/gM5n584j4IHAa8M6WzQc99zhx4kS6uroAGDduHOPHj2fC\nhAnA82/Ykbw8dy584xsTuPji4RHPoizPmDFjWMXjsssuu+yyy+1c7rnf3d1NpwzUM7YL8M7M3Ktc\n3h3YCvhwZr64fCwoRsxWbtl2S2BKZm5XLk8CFmTmUS3rjfqesW98A6ZN8yKvkiQNd3Vc2mImsGVE\nrFAWXdsANwO3R8Tby3XeAdzWy7bXAutFRFdELAvsAlzYprhHjCefhGOOgcmT645EkiTVod9iLDOv\nB86kKKxuAAL4HvBx4OiImAEcDnwMICLWjIhfl9s+C3wKuJiigDuv9ZOUgpNOgq23ho02qjuSxVMd\nxlWzmLtmM3/NZv5UNeB3U2bm0cDRLQ9fC7ypl3VnAztWln8D+PnAPjzxBBx7LFxxRd2RSJKkuvh1\nSDX6+tfhppvgrLPqjkSSJA2G3005gjz2GKy7Lvzxj/Ca19QdjSRJGgy/m3IEOeEE2H775hdi9j00\nl7lrNvPXbOZPVQP2jKn9Hn20KMb+9Ke6I5EkSXVzmrIGU6ZAdzecfnrNgUiSpEViz9gI8MgjsN56\ncM01sM46dUcjSZIWhT1jI8Bxx8HOO4+cQsy+h+Yyd81m/prN/KnKnrEhNGcOfOc7cO21dUciSZKG\nC6cph9BBB8FDD8Epp9QdiSRJWhz2jDXYP/9ZXMbiuuvgVa+qOxpJkrQ47BlrsGOOgf/5n5FXiNn3\n0FzmrtnMX7OZP1XZMzYE/vGPYmry+uvrjkSSJA03TlMOgS9+EebOhRNPrDsSSZK0JOwZa6AHH4TX\nvhZuuAHWWqvuaCRJ0pKwZ6yBjjoKdttt5BZi9j00l7lrNvPXbOZPVfaMddD99xdfefS3v9UdiSRJ\nGq6cpuyg/faDCPjmN+uORJIktYM9Yw1y333w+tfDzTfDGmvUHY0kSWoHe8Ya5MgjYc89R34hZt9D\nc5m7ZjN/zWb+VGXPWAfccw+cdRbMnFl3JJIkabhzmrID9tkHVlqp+CSlJEkaOewZa4BZs2CTTeDW\nW2G11eqORpIktZM9Yw3wta/Bxz42egox+x6ay9w1m/lrNvOnKnvG2qi7G376U7jttrojkSRJTeE0\nZRvttVfx6cnDD687EkmS1AmdmKZ0ZKxN7rwTfvELR8UkSdKisWesTQ4/HPbdF1Zdte5IhpZ9D81l\n7prN/DWb+VOVI2NtcMcdcOGFxU9JkqRFYc9YG3z0o7DOOnDwwXVHIkmSOsmesWHo1lvhooscFZMk\nSYvHnrEldNhhsN9+sPLKdUdSD/semsvcNZv5azbzp6oBR8YiYhLwYWABcCOwB3AmsH65yjjg0czc\npJdtu4HHgfnAvMzcoj1hDw+33AKXXALf+U7dkUiSpKbqt2csIrqA3wMbZubTEXEecFFmnlFZ5xiK\nYuwFV9eKiLuAN2bmnH6O0diesV13hTe8ASZNqjsSSZI0FOroGXscmAeMjYj5wFjgvkpAAfwPsHU/\n+2hrwMPFTTfB738Pp5xSdySSJKnJ+u0ZK0e0jgVmAbMpRsAuq6zyVuDBzPx7X7sALouIayNi73YE\nPFwceiihSzDDAAARk0lEQVTsvz+stFLdkdTLvofmMnfNZv6azfypqt+RsYhYB9gP6AIeA86PiN0y\n86xylV2Bs/vZxVsy8/6IeClwaUTMzMwrW1eaOHEiXV1dAIwbN47x48czYcIE4Pk37HBavvNOuOKK\nCZx22vCIp87lGTNmDKt4XHbZZZdddrmdyz33u7u76ZSBesZ2Ad6ZmXuVy7sDW2bmvhExBrgX2DQz\nZw94oIjJwJOZeWzL443rGfvAB2CrrYqRMUmSNHp0omdsqQGenwlsGRErlP1h2wI3l89tC9zSVyEW\nEWMjYqXy/ouAd1F8GrPRZsyAq66CT36y7kgkSdJI0G8xlpnXU1zG4lrghvLhnpb1XYBzqutHxJoR\n8etycQ3gyoiYAVwD/CozL2lX4HU55BA48EAYO7buSIaH6jCumsXcNZv5azbzp6oBrzOWmUcDR/fy\n+B69PDYb2LG8fycwvg0xDhvXXQfTpsHZZ9cdiSRJGin8bspFsNNOsO228JnP1B2JJEmqQyd6xizG\nBunaa+G97y2+g3L55euORpIk1aGOBn6VJk8urrRvIbYw+x6ay9w1m/lrNvOnqgF7xgTXXAM33ggX\nXFB3JJIkaaRxmnIQtt8edt4ZPvGJuiORJEl1quO7KUe9P/8Zbr4ZfvnLuiORJEkjkT1jA5g8Gb7y\nFVh22bojGZ7se2guc9ds5q/ZzJ+qLMb68cc/wu23w8SJdUciSZJGKnvG+rHNNrDbbrDnnnVHIkmS\nhgMvbTGErrgCurth993rjkSSJI1kFmN9mDwZvvpVWGaZuiMZ3ux7aC5z12zmr9nMn6osxnpx+eVw\n333w4Q/XHYkkSRrp7BlrkQlvfzvsvbdTlJIkaWH2jA2B3/0OHnwQdt217kgkSdJoYDFWkVn0ik2e\nDGO8HO6g2PfQXOau2cxfs5k/VVmMVVxyCTzyCOyyS92RSJKk0cKesVImbLUVfO5zFmOSJKl39ox1\n0G9/C08+CR/8YN2RSJKk0cRijGJU7OCDi16xpXxFFol9D81l7prN/DWb+VOVpQfw61/D00/Df/93\n3ZFIkqTRZtT3jGXCZpvBQQfB+99fWxiSJKkB7BnrgAsvhAUL4L3vrTsSSZI0Go3qYmzBgqJPbMoU\ne8UWl30PzWXums38NZv5U9WoLkF+8QtYemnYaae6I5EkSaPVqO0ZW7AAxo+Hr38d/uu/hvzwkiSp\ngewZa6Of/QyWXx523LHuSCRJ0mg2KouxBQvgkEOKXrFoa207+tj30FzmrtnMX7OZP1WNymLs/PNh\nxRVh++3rjkSSJI12o65nbP582Ggj+Na34N3vHrLDSpKkEcCesTY47zxYdVV417vqjkSSJGkQxVhE\nTIqImyLixog4OyKWi4jzImJ6ebsrIqb3se12ETEzIm6PiAPbH/6iefbZolfskEPsFWsX+x6ay9w1\nm/lrNvOnqjH9PRkRXcDewIaZ+XREnAd8KDN3qaxzDPBoL9suDZwIbAvcB/wlIi7MzFvaF/6iOecc\nWH112GabuiKQJElaWL89YxGxKvBnYEvgCeDnwPGZeVn5fAB3A1tn5t9btt0KmJyZ25XLXwLIzCNb\n1huSnrFnn4UNN4RTToGtt+744SRJ0gg05D1jmTkHOBaYBcwGHu0pxEpvBR5sLcRKawH3VJbvLR+r\nxY9/DGutZSEmSZKGl36LsYhYB9gP6ALWBFaMiN0qq+wKnN3H5vV+TLNi3jw47LCiV0ztZd9Dc5m7\nZjN/zWb+VNVvzxiwGfCnzHwYICIuAN4MnBURY4D3AZv2se19wCsry6+kGB17gYkTJ9LV1QXAuHHj\nGD9+PBMmTACef8MuyfJFF0FX1wTe/vb27M/l55dnzJgxrOJx2WWXXXbZ5XYu99zv7u6mUwbqGdsY\nOAvYHHgKOB2YlpknRcR2wIGZ2evEX1ms3QpsQzHFOQ3YtbWBv9M9Y888A695DfzoR/Cf/9mxw0iS\npFGgjp6x64EzgWuBG8qHTyl/7gKc0xLgmhHx63LbZ4FPARcDNwPn1fFJyjPOgPXWsxCTJEnD04i+\nAv8zzxSF2LnnwlZbdeQQo97UqVOfG9JVs5i7ZjN/zWb+mssr8C+i006D177WQkySJA1fI3Zk7Omn\ni1Gxn/4Uttii7buXJEmjkCNji+DUU+H1r7cQkyRJw9uILMaeegqOOMLrig2F6kd/1SzmrtnMX7OZ\nP1WNyGLslFNg001hs83qjkSSJKl/I65nbO5cWGcd+NWvioJMkiSpXewZG4TvfQ/e9CYLMUmS1Awj\nqhj797/hqKNgypS6Ixk97HtoLnPXbOav2cyfqkZUMfbd78Jb3gIbb1x3JJIkSYMzYnrG/vWvolfs\n0kuLS1pIkiS1mz1j/TjpJHjb2yzEJElSs4yIYuyJJ+DYY2Hy5LojGX3se2guc9ds5q/ZzJ+qRkQx\nduKJ8I53wOteV3ckkiRJi6bxPWOPPw7rrgt/+ANssEEbA5MkSWphz1gvvv1teNe7LMQkSVIzNboY\ne+wx+Na34OCD645k9LLvobnMXbOZv2Yzf6pqdDF2/PGwww6w/vp1RyJJkrR4Gtsz9uijRa/Y1VcX\nPyVJkjrNnrGKb34T3vMeCzFJktRsjSzG5swpLvL61a/WHYnse2guc9ds5q/ZzJ+qGlmMHXccvPe9\n8OpX1x2JJEnSkmlcz9jDDxcN+3/9K3R1dS4uSZKkVvaMAcccAx/4gIWYJEkaGRpVjD30EJxyChx0\nUN2RqId9D81l7prN/DWb+VNVo4qxY46BXXaBtdeuOxJJkqT2aEzP2D/+UXzl0Q03wCteMQSBSZIk\ntRjVPWNHHw277WYhJkmSRpZGFGMPPACnnQaTJtUdiVrZ99Bc5q7ZzF+zmT9VNaIYO+oo2H13WHPN\nuiORJElqr2HfMzZ7Nmy0Edx0E7z85UMYmCRJUotO9IwN+2LsM5+BMWOKq+5LkiTVqZYG/oiYFBE3\nRcSNEXF2RCxXPv7piLglIv4WEUf1sW13RNwQEdMjYtqiBnfffXDWWXDggYu6pYaKfQ/NZe6azfw1\nm/lT1Zj+noyILmBvYMPMfDoizgM+FBGzgJ2AN2TmvIh4aR+7SGBCZs5ZnOCOOAL23BNe9rLF2VqS\nJGn463eaMiJWBf4MbAk8AfwcOAHYC/heZv6+351H3AVslpkP97NOr9OU99wDG28MM2fC6qsP5lQk\nSZI6a8inKcsRrWOBWcBs4NHMvBRYH3hbRFwdEVMjYrO+dgFcFhHXRsTeixLY178OH/uYhZgkSRrZ\n+i3GImIdYD+gC1gTWDEidqOY3lwlM7cEvgj8pI9dvCUzNwG2B/aNiLcOJqi774af/AS+8IXBnYTq\nY99Dc5m7ZjN/zWb+VNVvzxiwGfCnnmnGiLgAeDNwL3ABQGb+JSIWRMRLWqcjM/P+8udDEfFzYAvg\nytaDTJw4ka6uLgDGjRvHZZeN5+Mfn8Bqqz3/hp0wYQLg8nBbnjFjxrCKx2WXXXbZZZfbudxzv7u7\nm04ZqGdsY+AsYHPgKeB0YBrwLLBmZk6OiPWByzJz7ZZtxwJLZ+YTEfEi4BLgkMy8pGW9hXrG7roL\nNtsMbrsNXvKSdpyiJElSe3SiZ6zfkbHMvD4izgSuBRYA1wGnlE+fFhE3As8AHykDXBP4fmbuCKwB\nXBARPcc5q7UQ683hh8M++1iISZKk0WFYXfT173+HN70Jbr8dVlml1rA0SFOnTn1uSFfNYu6azfw1\nm/lrrlou+jqUDjsMPvUpCzFJkjR6DJuRsdtvh622gjvugHHjag1JkiSpVyN6ZOyww+Czn7UQkyRJ\no8uwKMZuvRV+85uiGFOzVD/6q2Yxd81m/prN/KlqWBRjhx4Kn/scvPjFdUciSZI0tIZFz9hLX5r8\n/e+w0kq1hiJJktSvEdsz9vnPW4hJkqTRaVgUY5/6VN0RaHHZ99Bc5q7ZzF+zmT9VDYtibMUV645A\nkiSpHsOiZ6zuGCRJkgZjxPaMSZIkjVYWY1oi9j00l7lrNvPXbOZPVRZjkiRJNbJnTJIkaZDsGZMk\nSRphLMa0ROx7aC5z12zmr9nMn6osxiRJkmpkz5gkSdIg2TMmSZI0wliMaYnY99Bc5q7ZzF+zmT9V\nWYxJkiTVyJ4xSZKkQbJnTJIkaYSxGNMSse+hucxds5m/ZjN/qrIYkyRJqpE9Y5IkSYNkz5gkSdII\nYzGmJWLfQ3OZu2Yzf81m/lRlMSZJklQje8YkSZIGyZ4xSZKkEWbAYiwiJkXETRFxY0ScHRHLlY9/\nOiJuiYi/RcRRfWy7XUTMjIjbI+LAdgev+tn30FzmrtnMX7OZP1X1W4xFRBewN7BpZr4eWBr4UERs\nDewEvCEzNwKO6WXbpYETge2A1wK7RsSGbY1etZsxY0bdIWgxmbtmM3/NZv5UNdDI2OPAPGBsRIwB\nxgKzgU8AR2TmPIDMfKiXbbcA7sjM7nK9c4Gd2xa5hoVHH3207hC0mMxds5m/ZjN/quq3GMvMOcCx\nwCyKIuzRzLwUWB94W0RcHRFTI2KzXjZfC7insnxv+ZgkSZJKA01TrgPsB3QBawIrRsRuwBhglczc\nEvgi8JNeNvcjkqNAd3d33SFoMZm7ZjN/zWb+VNXvpS0iYhfgnZm5V7m8O7Al8GrgyMy8onz8DuBN\nmflwZdstgSmZuV25PAlYkJlHtRzDok2SJDVGuy9tMWaA52cCX42IFYCngG2BacANwDuAKyJifWDZ\naiFWuhZYr/wQwGxgF2DX1gO0+4QkSZKapN9iLDOvj4gzKQqrBcB1wCnl06dFxI3AM8BHACJiTeD7\nmbljZj4bEZ8CLqb4FOYPMvOWDp2HJElSI9V+BX5JkqTRrO1X4B/MhV4j4oTy+esjYpOBto2IVSPi\n0oi4LSIuiYhx7Y5bHcvdB8uLBs+PiE2H4jxGqw7l7xvlxZ2vj4gLImLloTiX0aZDuTusXHdGRPwu\nIl45FOcyGnUif5Xn94+IBRGxaifPYTTr0O/flIi4NyKml7ft+g0iM9t2o5iOvIPi05fLADOADVvW\n2QG4qLz/JuDqgbYFjgYOKO8fSPHhgbbGPtpvHczdBhSXQrmc4uLBtZ/rSLx1MH/vBJYq7x/p716j\ncrdSZftPA6fWfa4j8dap/JXPvxL4LXAXsGrd5zoSbx38/ZsMfH6wcbR7ZGwwF3rdCTgDIDOvAcZF\nxBoDbPvcNuXP97Y5bnUod5k5MzNvG6qTGMU6lb9LM3NBuf01wCs6fyqjTqdy90Rl+xWBf3b2NEat\nTv3dAzgOOKDTJzDKdTJ/g/6AYruLscFc6LWvddbsZ9uXZeaD5f0HgZe1K2A9p1O509AYivztCVy0\nxJGqVcdyFxFfi4hZwEcpRjbVfh3JX0TsDNybmTe0O2AtpJP/dn66nNb8wUDtVe0uxgb7aYDBVIvR\n2/6yGP/zUwft187caeh1NH8RcRDwTGaevTjbq18dy11mHpSZawOnA99c1O01KG3PX3k5qS9TTHUt\n8vZaJJ36/fsu8B/AeOB+im8z6tNA1xlbVPdRzHH3eCVFpdjfOq8o11mml8fvK+8/GBFrZOYDEfFy\n4B9tjVrQ3tz1tq06q2P5i4iJFD0T27QvXFUMxe/e2Tiq2SmdyN86FH1I10dEz/p/jYgtMtO/f+3V\nkd+/ap4i4lTg//qNos2NcGOAv1O8iZZl4Ea4LXm+Ea7PbSka+A8s738Jm4jbfutU7irbXg68se7z\nHKm3Dv7ubQfcBKxW9zmO1FsHc7deZftPAz+q+1xH4q3T/3aW69nA37D8AS+vbP854Ox+4+jAiW0P\n3ErxCYNJ5WMfBz5eWefE8vnrqXzCrrdty8dXBS4DbgMuAcbVncCReOtQ7t5HMac+F3gA+E3d5zlS\nbx3K3+3A3cD08vadus9zJN46lLufAjeWfyB+Bqxe93mO1Fsn8tey/zuxGGtU/oAzKb6t6HrgFxS9\n733G4EVfJUmSatT2i75KkiRp8CzGJEmSamQxJkmSVCOLMUmSpBpZjEmSJNXIYkySJKlGFmOSJEk1\nshiTJEmq0f8HojRgrg9T9McAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f5a6390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(regul_val,accuracy_val)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with beta value of  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-fdc88ce87b51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta_regularlization_nn\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mregul\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         _, l, predictions = session.run(\n\u001b[0;32m---> 21\u001b[0;31m           [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     22\u001b[0m       \u001b[0maccuracy_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/anaconda2/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 710\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    711\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/anaconda2/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 908\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/anaconda2/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 958\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Applications/anaconda/anaconda2/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/anaconda2/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [0,0.001,0.002,0.003,0.004,0.005]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.initialize_all_variables().run()\n",
    "      print(\"Initialized with beta value of \",regul)\n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,beta_regularlization : regul}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After plotting the result it is obvious that the value of beta = 0.001 is the most suitable for the model. This value can be now used for the 1 hidden layer model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization for a normal 1-layer NN implementation from Assignment 2 with SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_of_nodes = 1024\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    #Initializing the placeholder for the dataset\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels  = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset  = tf.constant(test_dataset)\n",
    "    ## variable for the regularization\n",
    "  beta_regularlization_nn = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "    # Init weights and bias for the hidden layer \n",
    "  hidden_layer_weights = tf.Variable(tf.truncated_normal([image_size * image_size , no_of_nodes]))\n",
    "  hidden_layer_bias    = tf.Variable(tf.zeros([no_of_nodes]))\n",
    "\n",
    "      # Init weights and bias for the output layer\n",
    "  output_layer_weights = tf.Variable(tf.truncated_normal([no_of_nodes , num_labels]))\n",
    "  output_layer_bias    = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Hidden layer using relu\n",
    "  hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_layer_weights)+hidden_layer_bias) \n",
    "    # Calculating the logits\n",
    "  logits = tf.matmul(hidden_layer,output_layer_weights) + output_layer_bias\n",
    "    # The loss using cross entropy and softmax\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits , tf_train_labels)) +\\\n",
    "                beta_regularlization_nn * (tf.nn.l2_loss(hidden_layer_weights) + tf.nn.l2_loss(output_layer_weights))\n",
    "\n",
    "    # Optimizer using GradientDesent\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  # Setup validation prediction step.        \n",
    "  valid_hidden_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_layer_weights) + hidden_layer_bias)       \n",
    "  valid_logits = tf.matmul(valid_hidden_layer, output_layer_weights) + output_layer_bias\n",
    "  valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "  # And setup the test prediction step.\n",
    "  test_hidden_layer = tf.nn.relu(tf.matmul(tf_test_dataset, hidden_layer_weights) + hidden_layer_bias)\n",
    "  test_logits = tf.matmul(test_hidden_layer, output_layer_weights) + output_layer_bias\n",
    "  test_prediction = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 670.021\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 23.5%\n",
      "Minibatch loss at step 500 : 199.045\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 1000 : 116.287\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 1500 : 68.8357\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 2000 : 41.219\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 2500 : 25.147\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 3000 : 15.4786\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.5%\n",
      "Test accuracy: 93.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "training_accuracy = []\n",
    "validation_accuracy = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in xrange(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regularlization_nn : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step\", step, \":\", l)\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "      training_accuracy.append(accuracy(predictions, batch_labels))\n",
    "      validation_accuracy.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAFCCAYAAAAHc1ITAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1109d5150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1,8)\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(x,training_accuracy,x,validation_accuracy)\n",
    "plt.axis([1,7,10,90])\n",
    "\n",
    "plt.grid(True)\n",
    "plt.title('Training accuracy and validation accuracy comparison for 1 hidden layer model')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with beta value of  0.001\n",
      "Initialized with beta value of  0.002\n",
      "Initialized with beta value of  0.003\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [0.001,0.002,0.003]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.initialize_all_variables().run()\n",
    "      print(\"Initialized with beta value of \",regul)\n",
    "      for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,beta_regularlization_nn : regul}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "      accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmYAAAFCCAYAAABb+RE3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiSpIkzMJEmSKsLETJIkqSL+P+XHzEvBvVG7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110b60690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(regul_val,accuracy_val)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious from the above plot, that the best value of beta is 0.001 as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 666.753\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 14.5%\n",
      "Minibatch loss at step 10 : 1079.64\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 19.4%\n",
      "Minibatch loss at step 20 : 360.356\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 21.6%\n",
      "Minibatch loss at step 30 : 356.77\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 21.6%\n",
      "Minibatch loss at step 40 : 353.219\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 21.6%\n",
      "Minibatch loss at step 50 : 349.703\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 21.6%\n",
      "Minibatch loss at step 60 : 346.223\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 21.6%\n",
      "Minibatch loss at step 70 : 342.777\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 21.5%\n",
      "Minibatch loss at step 80 : 339.366\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 21.5%\n",
      "Minibatch loss at step 90 : 335.988\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 21.5%\n",
      "Test accuracy: 22.4%\n"
     ]
    }
   ],
   "source": [
    "no_of_nodes = 1024\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    #Initializing the placeholder for the dataset\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels  = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset  = tf.constant(test_dataset)\n",
    "    ## variable for the regularization\n",
    "  beta_regularlization_nn = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "    # Init weights and bias for the hidden layer \n",
    "  hidden_layer_weights = tf.Variable(tf.truncated_normal([image_size * image_size , no_of_nodes]))\n",
    "  hidden_layer_bias    = tf.Variable(tf.zeros([no_of_nodes]))\n",
    "\n",
    "      # Init weights and bias for the output layer\n",
    "  output_layer_weights = tf.Variable(tf.truncated_normal([no_of_nodes , num_labels]))\n",
    "  output_layer_bias    = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Hidden layer using relu\n",
    "  hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_layer_weights)+hidden_layer_bias) \n",
    "    # Calculating the logits\n",
    "  logits = tf.matmul(hidden_layer,output_layer_weights) + output_layer_bias\n",
    "    # The loss using cross entropy and softmax\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits , tf_train_labels)) +\\\n",
    "                beta_regularlization_nn * (tf.nn.l2_loss(hidden_layer_weights) + tf.nn.l2_loss(output_layer_weights))\n",
    "\n",
    "    # Optimizer using GradientDesent\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  # Setup validation prediction step.        \n",
    "  valid_hidden_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_layer_weights) + hidden_layer_bias)       \n",
    "  valid_logits = tf.matmul(valid_hidden_layer, output_layer_weights) + output_layer_bias\n",
    "  valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "  # And setup the test prediction step.\n",
    "  test_hidden_layer = tf.nn.relu(tf.matmul(tf_test_dataset, hidden_layer_weights) + hidden_layer_bias)\n",
    "  test_logits = tf.matmul(test_hidden_layer, output_layer_weights) + output_layer_bias\n",
    "  test_prediction = tf.nn.softmax(test_logits)\n",
    "    \n",
    "    \n",
    "num_steps = 100\n",
    "batch_size = 5\n",
    "training_accuracy = []\n",
    "validation_accuracy = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in xrange(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step % batch_size) \n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regularlization_nn : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 10 == 0):\n",
    "      print(\"Minibatch loss at step\", step, \":\", l)\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "      training_accuracy.append(accuracy(predictions, batch_labels))\n",
    "      validation_accuracy.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAFCCAYAAADVI1hLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRESkH1RMiYiIiPSDiikRERGRfvj/wM8elaNjCwEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f4d04d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1,len(training_accuracy)+1)\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(x,training_accuracy,x,validation_accuracy)\n",
    "plt.axis([1,7,0,110])\n",
    "\n",
    "plt.grid(True)\n",
    "plt.title('Training accuracy and validation accuracy comparison for 1 hidden layer model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see the extreme case of overfitting when we limit out batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 793.893\n",
      "Minibatch accuracy: 20.0%\n",
      "Validation accuracy: 14.9%\n",
      "Minibatch loss at step 10 : 153712.0\n",
      "Minibatch accuracy: 60.0%\n",
      "Validation accuracy: 13.3%\n",
      "Minibatch loss at step 20 : 1.50438e+07\n",
      "Minibatch accuracy: 40.0%\n",
      "Validation accuracy: 11.8%\n",
      "Minibatch loss at step 30 : 2.29254e+07\n",
      "Minibatch accuracy: 60.0%\n",
      "Validation accuracy: 16.0%\n",
      "Minibatch loss at step 40 : 4.07516e+08\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 17.3%\n",
      "Minibatch loss at step 50 : 2.15249e+10\n",
      "Minibatch accuracy: 40.0%\n",
      "Validation accuracy: 10.9%\n",
      "Minibatch loss at step 60 : 2.42734e+11\n",
      "Minibatch accuracy: 20.0%\n",
      "Validation accuracy: 16.7%\n",
      "Minibatch loss at step 70 : 2.08566e+12\n",
      "Minibatch accuracy: 20.0%\n",
      "Validation accuracy: 13.7%\n",
      "Minibatch loss at step 80 : 8.21652e+12\n",
      "Minibatch accuracy: 20.0%\n",
      "Validation accuracy: 16.2%\n",
      "Minibatch loss at step 90 : 1.82357e+12\n",
      "Minibatch accuracy: 40.0%\n",
      "Validation accuracy: 17.3%\n",
      "Minibatch loss at step 100 : 6.51188e+12\n",
      "Minibatch accuracy: 40.0%\n",
      "Validation accuracy: 17.2%\n",
      "Test accuracy: 17.6%\n"
     ]
    }
   ],
   "source": [
    "no_of_nodes = 1024\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    #Initializing the placeholder for the dataset\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels  = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset  = tf.constant(test_dataset)\n",
    "    ## variable for the regularization\n",
    "  beta_regularlization_nn = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "    # Init weights and bias for the hidden layer \n",
    "  hidden_layer_weights = tf.Variable(tf.truncated_normal([image_size * image_size , no_of_nodes]))\n",
    "  hidden_layer_bias    = tf.Variable(tf.zeros([no_of_nodes]))\n",
    "\n",
    "      # Init weights and bias for the output layer\n",
    "  output_layer_weights = tf.Variable(tf.truncated_normal([no_of_nodes , num_labels]))\n",
    "  output_layer_bias    = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  \n",
    "    # Hidden layer using relu\n",
    "  hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_layer_weights)+hidden_layer_bias) \n",
    "  drop_out_l1 = tf.nn.dropout(hidden_layer, 0.5)\n",
    "    # Calculating the logits\n",
    "  logits = tf.matmul(drop_out_l1,output_layer_weights) + output_layer_bias\n",
    "    # The loss using cross entropy and softmax\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits , tf_train_labels)) +\\\n",
    "                beta_regularlization_nn * (tf.nn.l2_loss(hidden_layer_weights) + tf.nn.l2_loss(output_layer_weights))\n",
    "\n",
    "    # Optimizer using GradientDesent\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "  # Setup validation prediction step.        \n",
    "  valid_hidden_layer = tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_layer_weights) + hidden_layer_bias)       \n",
    "  valid_logits = tf.matmul(valid_hidden_layer, output_layer_weights) + output_layer_bias\n",
    "  valid_prediction = tf.nn.softmax(valid_logits)\n",
    "\n",
    "  # And setup the test prediction step.\n",
    "  test_hidden_layer = tf.nn.relu(tf.matmul(tf_test_dataset, hidden_layer_weights) + hidden_layer_bias)\n",
    "  test_logits = tf.matmul(test_hidden_layer, output_layer_weights) + output_layer_bias\n",
    "  test_prediction = tf.nn.softmax(test_logits)\n",
    "    \n",
    "    \n",
    "num_steps = 101\n",
    "batch_size = 5\n",
    "training_accuracy = []\n",
    "validation_accuracy = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in xrange(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step % batch_size) \n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regularlization_nn : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 10 == 0):\n",
    "      print(\"Minibatch loss at step\", step, \":\", l)\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "      training_accuracy.append(accuracy(predictions, batch_labels))\n",
    "      validation_accuracy.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2 hidden layer model with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "  ## variable for the regularization\n",
    "  beta_regularlization_nn = tf.placeholder(tf.float32)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "    \n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "    \n",
    "    \n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "      beta_regulularization_nn * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 3.35458\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 23.3%\n",
      "Minibatch loss at step 500 : 0.939529\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 1000 : 0.875601\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 1500 : 0.544009\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 2000 : 0.51685\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 2500 : 0.525977\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 3000 : 0.554578\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.6%\n",
      "Test accuracy: 94.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "training_accuracy = []\n",
    "validation_accuracy = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in xrange(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regularlization_nn : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step\", step, \":\", l)\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "      training_accuracy.append(accuracy(predictions, batch_labels))\n",
    "      validation_accuracy.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seem like with more layers and regularization the result is getting better\n",
    "### Lets try 3 hidden layers with regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "  beta_regularlization_nn = tf.placeholder(tf.float32)\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  \n",
    "\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  \n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  \n",
    "\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))+\\\n",
    "      beta_regularlization_nn * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3)+ tf.nn.l2_loss(weights4))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 3.42702\n",
      "Minibatch accuracy: 16.4%\n",
      "Validation accuracy: 37.2%\n",
      "Minibatch loss at step 2000 : 0.540107\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 4000 : 0.437594\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 6000 : 0.587153\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 8000 : 0.644813\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 10000 : 0.428871\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 12000 : 0.473604\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 14000 : 0.421187\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 16000 : 0.35204\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 18000 : 0.311133\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 20000 : 0.443139\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.7%\n",
      "Test accuracy: 95.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "training_accuracy = []\n",
    "validation_accuracy = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in xrange(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regularlization_nn : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2000 == 0):\n",
    "      print(\"Minibatch loss at step\", step, \":\", l)\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "      training_accuracy.append(accuracy(predictions, batch_labels))\n",
    "      validation_accuracy.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying four layers with regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "num_hidden_nodes4 = 64\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "  beta_regularlization_nn = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  \n",
    "\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  \n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  \n",
    "  \n",
    "\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_hidden_nodes4], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_hidden_nodes4]))\n",
    "    \n",
    "  weights5 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes4, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes4)))\n",
    "  biases5 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  lay4_train = tf.nn.relu(tf.matmul(lay3_train, weights4) + biases4)\n",
    "  logits = tf.matmul(lay4_train, weights5) + biases5\n",
    "  \n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))+\\\n",
    "      beta_regularlization_nn * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3)+ tf.nn.l2_loss(weights4) + tf.nn.l2_loss(weights5))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    \n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay4_valid = tf.nn.relu(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay4_valid, weights5) + biases5)\n",
    "  \n",
    "\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  lay4_test = tf.nn.relu(tf.matmul(lay3_test, weights4) + biases4)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay4_test, weights5) + biases5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 3.55934\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 20.6%\n",
      "Minibatch loss at step 2000 : 0.539295\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 4000 : 0.47813\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 6000 : 0.61889\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 8000 : 0.611202\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 10000 : 0.422707\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 12000 : 0.483692\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 14000 : 0.412324\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 16000 : 0.339105\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 18000 : 0.317888\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 20000 : 0.417661\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.4%\n",
      "Test accuracy: 95.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "training_accuracy = []\n",
    "validation_accuracy = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in xrange(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regularlization_nn : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 2000 == 0):\n",
    "        \n",
    "      print(\"Minibatch loss at step\", step, \":\", l)\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "      training_accuracy.append(accuracy(predictions, batch_labels))\n",
    "      validation_accuracy.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best test accuracy I could achieve so far is 95.9% by using 3 layers and regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
